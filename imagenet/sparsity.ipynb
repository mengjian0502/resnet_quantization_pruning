{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== conclusion ==== \n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-aa3d756d208b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"==== conclusion ==== \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m \u001b[0moverall_sparsity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcount_num_one\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mall_num\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'overall sparsity for all layer:{:.6f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverall_sparsity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0mgroup_sparsity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_num_one\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mall_num_group\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def weight_tern(fp_w, t_factor):\n",
    "    fp_w = torch.from_numpy(fp_w)\n",
    "    mean_w = fp_w.abs().mean()\n",
    "    max_w = fp_w.abs().max()\n",
    "    th = t_factor*max_w\n",
    "\n",
    "    output = fp_w.clone().zero_()\n",
    "    W = fp_w[fp_w.ge(th)+fp_w.le(-th)].abs().mean()\n",
    "    output[fp_w.ge(th)] = 1\n",
    "    output[fp_w.lt(-th)] = -1\n",
    "#     print('W:{}'.format(W))\n",
    "    return output.numpy()\n",
    "\n",
    "model_dir = '/Users/jianmeng/Desktop/ASU_research/analysis/exp_results/imagenet_test/resnet18_w4_a4_swpTrue_from_quant/checkpoint_{epoch}.pth.tar'\n",
    "# model = torch.load(model_dir, map_location=torch.device('cpu'))\n",
    "model = torch.load(model_dir)\n",
    "params = model['state_dict']\n",
    "\n",
    "counter = 0\n",
    "overall = 0\n",
    "num_one = 0\n",
    "all_num_one = 0.0\n",
    "all_num_group = 0.0\n",
    "group_ch = 16\n",
    "all_sparsity_mean = 0.0\n",
    "all_sparsity_min = 1.0\n",
    "all_num = 0.0\n",
    "count_num_one = 0.0\n",
    "t_factor = 0.05\n",
    "tern = True\n",
    "\n",
    "for k,v in params.items():\n",
    "    if 'stage' in k and 'conv' in k:\n",
    "        print(k)\n",
    "        \n",
    "        counter += 1\n",
    "        conv_w = v \n",
    "        output = np.array(conv_w)\n",
    "        cout = output.shape[0]\n",
    "        cin = output.shape[1]\n",
    "        kh = output.shape[2]\n",
    "        kw = output.shape[3]\n",
    "        \n",
    "        if tern:\n",
    "            out_sparse = weight_tern(output, t_factor)\n",
    "        else:\n",
    "        # == set threshold for weight\n",
    "            out_sparse = np.zeros(output.shape)\n",
    "            out_sparse[np.absolute(output) > 1e-3] = output[np.absolute(output) > 1e-3]\n",
    "        # ==\n",
    "        \n",
    "        #cal the whole number of weight in current layer\n",
    "        count_num_layer = cout * cin * kh * kw\n",
    "        all_num += count_num_layer\n",
    "        #cal the nonzero weight in current layer\n",
    "        count_one_layer = np.count_nonzero(out_sparse)\n",
    "        count_num_one += count_one_layer\n",
    "        \n",
    "        w_t = out_sparse.reshape(cout, cin // group_ch, group_ch, kh, kw)\n",
    "        num_group = (cout * cin) // group_ch\n",
    "        w_t = w_t.reshape(num_group, group_ch, kh, kw)\n",
    "        print(f\"{k}, The shape of w_t: {w_t.shape}\")\n",
    "        all_num_group += num_group\n",
    "        sparsity_chall = np.zeros(num_group)\n",
    "        num_one = 0\n",
    "        for i in range(num_group):       \n",
    "            sparsity_group = 1.0*(w_t[i].size - np.count_nonzero(w_t[i]))/w_t[i].size\n",
    "            sparsity_chall[i] = sparsity_group\n",
    "            if np.count_nonzero(w_t[i]) is 0:\n",
    "                num_one += 1\n",
    "        all_num_one += num_one \n",
    "        print('num of sparse group:{}'.format(num_one))\n",
    "        print('num of group:{}'.format(num_group))\n",
    "\n",
    "print(\"==== conclusion ==== \")\n",
    "overall_sparsity = 1 - count_num_one/all_num\n",
    "print('overall sparsity for all layer:{:.6f}'.format(overall_sparsity))\n",
    "group_sparsity = all_num_one/all_num_group\n",
    "print('group sparsity for all layer:{:.6f}'.format(group_sparsity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randint(5, size=(2, 4))\n",
    "b = np.count_nonzero(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
