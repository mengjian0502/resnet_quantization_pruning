{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "#                                                                                                                                                                 import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']= '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4948, 0.5271, 0.6639, 0.7366, 0.9027]) tensor([0, 1, 2, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(5)\n",
    "b,c = a.sort()\n",
    "print(b,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "channel-wise pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "torch.Size([64, 3, 7, 7])\n",
      "quanConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "torch.Size([64, 64, 3, 3])\n",
      "quanConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "torch.Size([64, 64, 3, 3])\n",
      "quanConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "torch.Size([64, 64, 3, 3])\n",
      "quanConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "torch.Size([64, 64, 3, 3])\n",
      "quanConv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "torch.Size([128, 64, 3, 3])\n",
      "quanConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "torch.Size([128, 128, 3, 3])\n",
      "quanConv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "torch.Size([128, 64, 1, 1])\n",
      "quanConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "torch.Size([128, 128, 3, 3])\n",
      "quanConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "torch.Size([128, 128, 3, 3])\n",
      "quanConv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "torch.Size([256, 128, 3, 3])\n",
      "quanConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "torch.Size([256, 256, 3, 3])\n",
      "quanConv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "torch.Size([256, 128, 1, 1])\n",
      "quanConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "torch.Size([256, 256, 3, 3])\n",
      "quanConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "torch.Size([256, 256, 3, 3])\n",
      "quanConv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "torch.Size([512, 256, 3, 3])\n",
      "quanConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "torch.Size([512, 512, 3, 3])\n",
      "quanConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "torch.Size([512, 256, 1, 1])\n",
      "quanConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "torch.Size([512, 512, 3, 3])\n",
      "quanConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "torch.Size([512, 512, 3, 3])\n",
      "Linear(in_features=512, out_features=1000, bias=True)\n",
      "torch.Size([1000, 512])\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "import torch.nn as nn\n",
    "from models.ResNet_tern import resnet18b_ff_lf_tex1, resnet18b_fq_lq_tex1\n",
    "\n",
    "\n",
    "net = resnet18b_ff_lf_tex1()\n",
    "\n",
    "new_state_dict = OrderedDict()\n",
    "\n",
    "model = torch.load('./save/resnet18/resnet18_00005_07/checkpoint.pth.tar')\n",
    "\n",
    "params = model['state_dict']\n",
    "\n",
    "for k,v in params.items():\n",
    "    name = k[7:]\n",
    "    new_state_dict[name] = v\n",
    "\n",
    "net.load_state_dict(new_state_dict)\n",
    "for m in net.modules():\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        print(m)\n",
    "        print(m.weight.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of filters:64\n",
      "num of sparse filters:25\n",
      "Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) filter wise sparsity_mean: 0.683567\t sparsity_min: 0.258503\n",
      "num of filters:64\n",
      "num of sparse filters:0\n",
      "quanConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) filter wise sparsity_mean: 0.830594\t sparsity_min: 0.630208\n",
      "num of filters:64\n",
      "num of sparse filters:1\n",
      "quanConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) filter wise sparsity_mean: 0.712511\t sparsity_min: 0.392361\n",
      "num of filters:64\n",
      "num of sparse filters:0\n",
      "quanConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) filter wise sparsity_mean: 0.835395\t sparsity_min: 0.652778\n",
      "num of filters:64\n",
      "num of sparse filters:5\n",
      "quanConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) filter wise sparsity_mean: 0.801188\t sparsity_min: 0.581597\n",
      "num of filters:128\n",
      "num of sparse filters:0\n",
      "quanConv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) filter wise sparsity_mean: 0.740099\t sparsity_min: 0.559028\n",
      "num of filters:128\n",
      "num of sparse filters:1\n",
      "quanConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) filter wise sparsity_mean: 0.874925\t sparsity_min: 0.688368\n",
      "num of filters:128\n",
      "num of sparse filters:30\n",
      "quanConv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) filter wise sparsity_mean: 0.837769\t sparsity_min: 0.343750\n",
      "num of filters:128\n",
      "num of sparse filters:2\n",
      "quanConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) filter wise sparsity_mean: 0.865221\t sparsity_min: 0.727431\n",
      "num of filters:128\n",
      "num of sparse filters:15\n",
      "quanConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) filter wise sparsity_mean: 0.892924\t sparsity_min: 0.585069\n",
      "num of filters:256\n",
      "num of sparse filters:1\n",
      "quanConv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) filter wise sparsity_mean: 0.903731\t sparsity_min: 0.780382\n",
      "num of filters:256\n",
      "num of sparse filters:11\n",
      "quanConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) filter wise sparsity_mean: 0.863909\t sparsity_min: 0.700087\n",
      "num of filters:256\n",
      "num of sparse filters:98\n",
      "quanConv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) filter wise sparsity_mean: 0.914276\t sparsity_min: 0.460938\n",
      "num of filters:256\n",
      "num of sparse filters:8\n",
      "quanConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) filter wise sparsity_mean: 0.894762\t sparsity_min: 0.713542\n",
      "num of filters:256\n",
      "num of sparse filters:36\n",
      "quanConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) filter wise sparsity_mean: 0.940086\t sparsity_min: 0.697483\n",
      "num of filters:512\n",
      "num of sparse filters:0\n",
      "quanConv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) filter wise sparsity_mean: 0.900965\t sparsity_min: 0.801215\n",
      "num of filters:512\n",
      "num of sparse filters:0\n",
      "quanConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) filter wise sparsity_mean: 0.962283\t sparsity_min: 0.849826\n",
      "num of filters:512\n",
      "num of sparse filters:153\n",
      "quanConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) filter wise sparsity_mean: 0.967026\t sparsity_min: 0.757812\n",
      "num of filters:512\n",
      "num of sparse filters:0\n",
      "quanConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) filter wise sparsity_mean: 0.952175\t sparsity_min: 0.902127\n",
      "num of filters:512\n",
      "num of sparse filters:0\n",
      "quanConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) filter wise sparsity_mean: 0.958528\t sparsity_min: 0.910373\n",
      "num of filters:1000\n",
      "num of sparse filters:0\n",
      "Linear(in_features=512, out_features=1000, bias=True) filter wise sparsity_mean: 0.326973\t sparsity_min: 0.216797\n",
      "==== conclusion ==== \n",
      "all_sparsity_min:0.216797\n",
      "group density for all layer:0.933448\n",
      "compression rate:21.885442\n"
     ]
    }
   ],
   "source": [
    "def weight_tern(fp_w, t_factor):\n",
    "    fp_w = torch.from_numpy(fp_w)\n",
    "    mean_w = fp_w.abs().mean()\n",
    "    max_w = fp_w.abs().max()\n",
    "    th = t_factor*max_w\n",
    "\n",
    "    output = fp_w.clone().zero_()\n",
    "    W = fp_w[fp_w.ge(th)+fp_w.le(-th)].abs().mean()\n",
    "    output[fp_w.ge(th)] = 1\n",
    "    output[fp_w.lt(-th)] = -1\n",
    "#     print('W:{}'.format(W))\n",
    "    return output.numpy()\n",
    "\n",
    "\n",
    "# model = torch.load('./save/2019-06-10/cifar10_resnet20_120_swp_alllayer_channelwise/model_best.pth.tar')\n",
    "# params = model['state_dict']\n",
    "\n",
    "counter = 0\n",
    "overall = 0\n",
    "num_one = 0\n",
    "all_num_one = 0.0\n",
    "all_num_group = 0.0\n",
    "all_sparsity_mean = 0.0\n",
    "all_sparsity_min = 1.0\n",
    "\n",
    "t_factor = 0.05\n",
    "tern = True\n",
    "\n",
    "for m in net.modules():\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "\n",
    "        counter += 1\n",
    "        conv_w = m.weight.data\n",
    "        output = np.array(conv_w)\n",
    "        cout = output.shape[0]\n",
    "        cin = output.shape[1]\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            kh = output.shape[2]\n",
    "            kw = output.shape[3]\n",
    "        else:\n",
    "            kh = 1\n",
    "            kw = 1\n",
    "            \n",
    "        if tern:\n",
    "            out_sparse = weight_tern(output, t_factor)\n",
    "        else:\n",
    "        # == set threshold for weight\n",
    "            out_sparse = np.zeros(output.shape)\n",
    "            out_sparse[np.absolute(output) > 1e-2] = output[np.absolute(output) > 1e-2]\n",
    "        # ==\n",
    "        w_t = out_sparse.reshape(cout, cin * kh * kw)\n",
    "        num_group = cout\n",
    "#         print(w_t.shape)\n",
    "        all_num_group += num_group\n",
    "        sparsity_chall = np.zeros(num_group)\n",
    "        num_one = 0\n",
    "        for i in range(num_group):       \n",
    "            sparsity_group = 1.0*(w_t[i].size - np.count_nonzero(w_t[i]))/w_t[i].size\n",
    "            sparsity_chall[i] = sparsity_group\n",
    "#             print('group wise {} sparsity: {:.8f}'.format(i, sparsity_group))\n",
    "            if np.count_nonzero(w_t[i]) is 0:\n",
    "                num_one += 1\n",
    "        all_num_one += num_one \n",
    "        print('num of filters:{}'.format(cout))\n",
    "\n",
    "        print('num of sparse filters:{}'.format(num_one))\n",
    "        \n",
    "        #sparsity mean and min\n",
    "        sparsity_mean = np.mean(sparsity_chall)\n",
    "        sparsity_min = np.min(sparsity_chall[sparsity_chall < 1.0])\n",
    "        all_sparsity_mean += sparsity_mean\n",
    "        if sparsity_min < all_sparsity_min:\n",
    "            all_sparsity_min = sparsity_min\n",
    "        print('{} filter wise sparsity_mean: {:.6f}\\t sparsity_min: {:.6f}'.format(m, sparsity_mean, sparsity_min))\n",
    "\n",
    "print(\"==== conclusion ==== \")\n",
    "print('all_sparsity_min:{:.6f}'.format(all_sparsity_min))\n",
    "density = 1 - all_num_one/all_num_group\n",
    "print('group density for all layer:{:.6f}'.format(1 - all_num_one/all_num_group))\n",
    "cp_r = 16 / ((density)*(1.0 - all_sparsity_min))\n",
    "print('compression rate:{:.6f}'.format(cp_r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "without tern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 16, 3, 3)\n",
      "num of sparse group:0\n",
      "num of group:16\n",
      "stage_1.0.conv_a.weight filter wise sparsity_mean: 0.197917\t sparsity_min: 0.131944\n",
      "(16, 16, 3, 3)\n",
      "num of sparse group:1\n",
      "num of group:16\n",
      "stage_1.0.conv_b.weight filter wise sparsity_mean: 0.101128\t sparsity_min: 0.006944\n",
      "(16, 16, 3, 3)\n",
      "num of sparse group:3\n",
      "num of group:16\n",
      "stage_1.1.conv_a.weight filter wise sparsity_mean: 0.216580\t sparsity_min: 0.013889\n",
      "(16, 16, 3, 3)\n",
      "num of sparse group:5\n",
      "num of group:16\n",
      "stage_1.1.conv_b.weight filter wise sparsity_mean: 0.434896\t sparsity_min: 0.131944\n",
      "(16, 16, 3, 3)\n",
      "num of sparse group:2\n",
      "num of group:16\n",
      "stage_1.2.conv_a.weight filter wise sparsity_mean: 0.163194\t sparsity_min: 0.006944\n",
      "(16, 16, 3, 3)\n",
      "num of sparse group:5\n",
      "num of group:16\n",
      "stage_1.2.conv_b.weight filter wise sparsity_mean: 0.396701\t sparsity_min: 0.062500\n",
      "(32, 16, 3, 3)\n",
      "num of sparse group:4\n",
      "num of group:32\n",
      "stage_2.0.conv_a.weight filter wise sparsity_mean: 0.161458\t sparsity_min: 0.000000\n",
      "(64, 16, 3, 3)\n",
      "num of sparse group:7\n",
      "num of group:64\n",
      "stage_2.0.conv_b.weight filter wise sparsity_mean: 0.268446\t sparsity_min: 0.069444\n",
      "(64, 16, 3, 3)\n",
      "num of sparse group:27\n",
      "num of group:64\n",
      "stage_2.1.conv_a.weight filter wise sparsity_mean: 0.436957\t sparsity_min: 0.006944\n",
      "(64, 16, 3, 3)\n",
      "num of sparse group:11\n",
      "num of group:64\n",
      "stage_2.1.conv_b.weight filter wise sparsity_mean: 0.241428\t sparsity_min: 0.013889\n",
      "(64, 16, 3, 3)\n",
      "num of sparse group:10\n",
      "num of group:64\n",
      "stage_2.2.conv_a.weight filter wise sparsity_mean: 0.305556\t sparsity_min: 0.006944\n",
      "(64, 16, 3, 3)\n",
      "num of sparse group:21\n",
      "num of group:64\n",
      "stage_2.2.conv_b.weight filter wise sparsity_mean: 0.373264\t sparsity_min: 0.006944\n",
      "(128, 16, 3, 3)\n",
      "num of sparse group:26\n",
      "num of group:128\n",
      "stage_3.0.conv_a.weight filter wise sparsity_mean: 0.317491\t sparsity_min: 0.000000\n",
      "(256, 16, 3, 3)\n",
      "num of sparse group:80\n",
      "num of group:256\n",
      "stage_3.0.conv_b.weight filter wise sparsity_mean: 0.383219\t sparsity_min: 0.006944\n",
      "(256, 16, 3, 3)\n",
      "num of sparse group:72\n",
      "num of group:256\n",
      "stage_3.1.conv_a.weight filter wise sparsity_mean: 0.370361\t sparsity_min: 0.006944\n",
      "(256, 16, 3, 3)\n",
      "num of sparse group:105\n",
      "num of group:256\n",
      "stage_3.1.conv_b.weight filter wise sparsity_mean: 0.477648\t sparsity_min: 0.006944\n",
      "(256, 16, 3, 3)\n",
      "num of sparse group:81\n",
      "num of group:256\n",
      "stage_3.2.conv_a.weight filter wise sparsity_mean: 0.371745\t sparsity_min: 0.000000\n",
      "(256, 16, 3, 3)\n",
      "num of sparse group:114\n",
      "num of group:256\n",
      "stage_3.2.conv_b.weight filter wise sparsity_mean: 0.628309\t sparsity_min: 0.159722\n",
      "==== conclusion ==== \n",
      "all_sparsity_min:0.000000\n",
      "group density for all layer:0.690733\n",
      "compression rate:23.163807\n"
     ]
    }
   ],
   "source": [
    "def weight_tern(fp_w, t_factor):\n",
    "    fp_w = torch.from_numpy(fp_w)\n",
    "    mean_w = fp_w.abs().mean()\n",
    "    max_w = fp_w.abs().max()\n",
    "    th = t_factor*max_w\n",
    "\n",
    "    output = fp_w.clone().zero_()\n",
    "    W = fp_w[fp_w.ge(th)+fp_w.le(-th)].abs().mean()\n",
    "    output[fp_w.ge(th)] = 1\n",
    "    output[fp_w.lt(-th)] = -1\n",
    "#     print('W:{}'.format(W))\n",
    "    return output.numpy()\n",
    "\n",
    "\n",
    "model = torch.load('./save/2019-05-29/cifar10_resnet20_120_base_swp_0002/checkpoint.pth.tar')\n",
    "params = model['state_dict']\n",
    "\n",
    "counter = 0\n",
    "overall = 0\n",
    "num_one = 0\n",
    "all_num_one = 0.0\n",
    "all_num_group = 0.0\n",
    "group_ch = 16\n",
    "all_sparsity_mean = 0.0\n",
    "all_sparsity_min = 1.0\n",
    "\n",
    "t_factor = 0.05\n",
    "tern = False\n",
    "\n",
    "for k,v in params.items():\n",
    "#     print(k)\n",
    "    if 'stage' in k and 'conv' in k:\n",
    "#     if 'layer' in k and 'conv' in k:\n",
    "#         print(v)\n",
    "        counter += 1\n",
    "        conv_w = v \n",
    "        output = np.array(conv_w)\n",
    "        cout = output.shape[0]\n",
    "        cin = output.shape[1]\n",
    "        kh = output.shape[2]\n",
    "        kw = output.shape[3]\n",
    "        \n",
    "        if tern:\n",
    "            out_sparse = weight_tern(output, t_factor)\n",
    "        else:\n",
    "        # == set threshold for weight\n",
    "            out_sparse = np.zeros(output.shape)\n",
    "            out_sparse[np.absolute(output) > 1e-3] = output[np.absolute(output) > 1e-3]\n",
    "        # ==\n",
    "        \n",
    "        w_t = out_sparse.reshape(cout, cin // group_ch, group_ch, 3, 3)\n",
    "        num_group = (cout * cin) // group_ch\n",
    "        w_t = w_t.reshape(num_group, group_ch, 3, 3)\n",
    "        print(w_t.shape)\n",
    "        all_num_group += num_group\n",
    "        sparsity_chall = np.zeros(num_group)\n",
    "        num_one = 0\n",
    "        for i in range(num_group):       \n",
    "            sparsity_group = 1.0*(w_t[i].size - np.count_nonzero(w_t[i]))/w_t[i].size\n",
    "            sparsity_chall[i] = sparsity_group\n",
    "#             print('group wise {} sparsity: {:.8f}'.format(i, sparsity_group))\n",
    "            if np.count_nonzero(w_t[i]) is 0:\n",
    "                num_one += 1\n",
    "        all_num_one += num_one \n",
    "        print('num of sparse group:{}'.format(num_one))\n",
    "        print('num of group:{}'.format(num_group))\n",
    "        \n",
    "        #sparsity mean and min\n",
    "        sparsity_mean = np.mean(sparsity_chall)\n",
    "        sparsity_min = np.min(sparsity_chall[sparsity_chall < 1.0])\n",
    "        all_sparsity_mean += sparsity_mean\n",
    "        if sparsity_min < all_sparsity_min:\n",
    "            all_sparsity_min = sparsity_min\n",
    "        print('{} filter wise sparsity_mean: {:.6f}\\t sparsity_min: {:.6f}'.format(k, sparsity_mean, sparsity_min))\n",
    "\n",
    "print(\"==== conclusion ==== \")\n",
    "print('all_sparsity_min:{:.6f}'.format(all_sparsity_min))\n",
    "density = 1 - all_num_one/all_num_group\n",
    "print('group density for all layer:{:.6f}'.format(1 - all_num_one/all_num_group))\n",
    "cp_r = 16 / ((density)*(1.0 - all_sparsity_min))\n",
    "print('compression rate:{:.6f}'.format(cp_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 3, 3)\n",
      "num of sparse group:50\n",
      "num of group:128\n",
      "module.chained_blocks.0.dw_conv.weight filter wise sparsity_mean: 0.551215\t sparsity_min: 0.000000\n",
      "(64, 128)\n",
      "num of sparse group:1\n",
      "num of group:64\n",
      "module.chained_blocks.0.pw_conv.weight filter wise sparsity_mean: 0.649170\t sparsity_min: 0.398438\n",
      "(128, 3, 3)\n",
      "num of sparse group:10\n",
      "num of group:128\n",
      "module.chained_blocks.1.dw_conv.weight filter wise sparsity_mean: 0.379340\t sparsity_min: 0.000000\n",
      "(64, 128)\n",
      "num of sparse group:0\n",
      "num of group:64\n",
      "module.chained_blocks.1.pw_conv.weight filter wise sparsity_mean: 0.460327\t sparsity_min: 0.273438\n",
      "(128, 3, 3)\n",
      "num of sparse group:1\n",
      "num of group:128\n",
      "module.chained_blocks.2.dw_conv.weight filter wise sparsity_mean: 0.333333\t sparsity_min: 0.000000\n",
      "(64, 128)\n",
      "num of sparse group:0\n",
      "num of group:64\n",
      "module.chained_blocks.2.pw_conv.weight filter wise sparsity_mean: 0.399780\t sparsity_min: 0.171875\n",
      "==== conclusion ==== \n",
      "all_sparsity_min:0.000000\n",
      "group density for all layer:0.892361\n",
      "compression rate:17.929961\n"
     ]
    }
   ],
   "source": [
    "def weight_tern(fp_w, t_factor):\n",
    "    fp_w = torch.from_numpy(fp_w)\n",
    "    mean_w = fp_w.abs().mean()\n",
    "    max_w = fp_w.abs().max()\n",
    "    th = t_factor*max_w\n",
    "\n",
    "    output = fp_w.clone().zero_()\n",
    "    W = fp_w[fp_w.ge(th)+fp_w.le(-th)].abs().mean()\n",
    "    output[fp_w.ge(th)] = 1\n",
    "    output[fp_w.lt(-th)] = -1\n",
    "#     print('W:{}'.format(W))\n",
    "    return output.numpy()\n",
    "\n",
    "\n",
    "# checkpoint = torch.load('./save/2019-05-12/mnist_bd_net_100_mnist_tt_32_bs256_m2_128_d3_newlr_th005_drop1_swp00005/model_best.pth.tar')\n",
    "# checkpoint = torch.load('./save/2019-05-12/svhn_bd_net_100_svhn_tt_32_bs256_m2_256_64_d3_newlr_drop1_swp00002/model_best.pth.tar')\n",
    "# checkpoint = torch.load('./save/2019-05-13/svhn_bd_net_100_svhn_tt_32_bs256_m2_256_64_d3_newlr_drop1_swp00001/model_best.pth.tar')\n",
    "# checkpoint = torch.load('./save/2019-05-13/svhn_bd_net_100_svhn_tt_32_bs256_m2_256_64_d3_newlr_drop1_newswp00001/model_best.pth.tar')\n",
    "# checkpoint = torch.load('./save/2019-05-13/svhn_bd_net_100_svhn_tt_32_bs256_m2_256_64_d3_newlr_drop1_newswp00005_finetune/checkpoint.pth.tar')\n",
    "checkpoint = torch.load('./save/2019-05-13/svhn_bd_net_100_svhn_tt_32_bs256_m2_256_64_d3_newlr005_drop1_newswp00002_05mean_finetune/checkpoint.pth.tar')\n",
    "\n",
    "params = checkpoint['state_dict']\n",
    "\n",
    "counter = 0\n",
    "overall = 0\n",
    "num_one = 0\n",
    "all_num_one = 0.0\n",
    "all_num_group = 0.0\n",
    "all_sparsity_mean = 0.0\n",
    "all_sparsity_min = 1.0\n",
    "\n",
    "t_factor = 0.05\n",
    "\n",
    "for k,v in params.items():\n",
    "#     print(k)\n",
    "    if \"dw_conv.weight\" in k:\n",
    "        tern = True\n",
    "\n",
    "        counter += 1\n",
    "        conv_w = v \n",
    "        output = np.array(conv_w)\n",
    "        cout = output.shape[0]\n",
    "        cin = output.shape[1]\n",
    "        kh = output.shape[2]\n",
    "        kw = output.shape[3]\n",
    "        \n",
    "        if tern:\n",
    "            out_sparse = weight_tern(output, t_factor)\n",
    "        else:\n",
    "        # == set threshold for weight\n",
    "            out_sparse = np.zeros(output.shape)\n",
    "            out_sparse[np.absolute(output) > 1e-5] = output[np.absolute(output) > 1e-5]\n",
    "        # ==\n",
    "        \n",
    "        w_t = np.array(out_sparse)\n",
    "        num_group = w_t.shape[0]\n",
    "        w_t = w_t.reshape(num_group, 3, 3)\n",
    "        print(w_t.shape)\n",
    "        all_num_group += num_group\n",
    "        sparsity_chall = np.zeros(num_group)\n",
    "        num_one = 0\n",
    "        for i in range(num_group):       \n",
    "#             print(w_t[i])\n",
    "            sparsity_group = 1.0*(w_t[i].size - np.count_nonzero(w_t[i]))/w_t[i].size\n",
    "            sparsity_chall[i] = sparsity_group\n",
    "#             print('group wise {} sparsity: {:.8f}'.format(i, sparsity_group))\n",
    "            if np.count_nonzero(w_t[i]) is 0:\n",
    "                num_one += 1\n",
    "        all_num_one += num_one \n",
    "        print('num of sparse group:{}'.format(num_one))\n",
    "        print('num of group:{}'.format(num_group))\n",
    "        \n",
    "        #sparsity mean and min\n",
    "        sparsity_mean = np.mean(sparsity_chall)\n",
    "        sparsity_min = np.min(sparsity_chall[sparsity_chall < 1.0])\n",
    "        all_sparsity_mean += sparsity_mean\n",
    "        if sparsity_min < all_sparsity_min:\n",
    "            all_sparsity_min = sparsity_min\n",
    "        print('{} filter wise sparsity_mean: {:.6f}\\t sparsity_min: {:.6f}'.format(k, sparsity_mean, sparsity_min))\n",
    "\n",
    "    if \"pw_conv.weight\" in k:\n",
    "        tern = True\n",
    "\n",
    "        counter += 1\n",
    "        conv_w = v \n",
    "        output = np.array(conv_w)\n",
    "        cout = output.shape[0]\n",
    "        cin = output.shape[1]\n",
    "        kh = output.shape[2]\n",
    "        kw = output.shape[3]\n",
    "        \n",
    "        if tern:\n",
    "            out_sparse = weight_tern(output, t_factor)\n",
    "        else:\n",
    "        # == set threshold for weight\n",
    "            out_sparse = np.zeros(output.shape)\n",
    "            out_sparse[np.absolute(output) > 1e-3] = output[np.absolute(output) > 1e-3]\n",
    "        # ==\n",
    "        \n",
    "        w_t = np.array(out_sparse)\n",
    "        num_group = w_t.shape[0]\n",
    "        w_t = w_t.reshape(num_group, w_t.shape[1])\n",
    "        print(w_t.shape)\n",
    "        all_num_group += num_group\n",
    "        sparsity_chall = np.zeros(num_group)\n",
    "        num_one = 0\n",
    "        for i in range(num_group):       \n",
    "#             print(w_t[i])\n",
    "            sparsity_group = 1.0*(w_t[i].size - np.count_nonzero(w_t[i]))/w_t[i].size\n",
    "            sparsity_chall[i] = sparsity_group\n",
    "#             print('group wise {} sparsity: {:.8f}'.format(i, sparsity_group))\n",
    "            if np.count_nonzero(w_t[i]) is 0:\n",
    "                num_one += 1\n",
    "        all_num_one += num_one \n",
    "        print('num of sparse group:{}'.format(num_one))\n",
    "        print('num of group:{}'.format(num_group))\n",
    "        \n",
    "        #sparsity mean and min\n",
    "        sparsity_mean = np.mean(sparsity_chall)\n",
    "        sparsity_min = np.min(sparsity_chall[sparsity_chall < 1.0])\n",
    "        all_sparsity_mean += sparsity_mean\n",
    "        if sparsity_min < all_sparsity_min:\n",
    "            all_sparsity_min = sparsity_min\n",
    "        print('{} filter wise sparsity_mean: {:.6f}\\t sparsity_min: {:.6f}'.format(k, sparsity_mean, sparsity_min))\n",
    "\n",
    "print(\"==== conclusion ==== \")\n",
    "print('all_sparsity_min:{:.6f}'.format(all_sparsity_min))\n",
    "density = 1 - all_num_one/all_num_group\n",
    "print('group density for all layer:{:.6f}'.format(1 - all_num_one/all_num_group))\n",
    "cp_r = 16 / ((density)*(1.0 - all_sparsity_min))\n",
    "print('compression rate:{:.6f}'.format(cp_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
